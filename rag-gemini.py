import os
import json
import tempfile
import shutil
import streamlit as st

from langchain_community.document_loaders import (
    TextLoader, PDFMinerLoader, UnstructuredWordDocumentLoader, UnstructuredMarkdownLoader,
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai.llms import GoogleGenerativeAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_core.documents import Document

# ------------------------ #
# Configs
# ------------------------ #
GEMINI_API_KEY = "AIzaSyBUAYGkxXkMxQ_bQ6mqgwqCmEwieBRtD8c"
os.environ["GOOGLE_API_KEY"] = GEMINI_API_KEY

DB_FOLDER = "faiss_index"

st.set_page_config(page_title="RAG FAISS + Gemini", page_icon="üîç", layout="wide")
st.title("üîç RAG App with Gemini")

# ------------------------ #
# Functions
# ------------------------ #

def load_uploaded_documents(uploaded_files):
    documents = []
    loaders = {
        ".txt": TextLoader,
        ".pdf": PDFMinerLoader,
        ".docx": UnstructuredWordDocumentLoader,
        ".md": UnstructuredMarkdownLoader
    }
    for uploaded_file in uploaded_files:
        suffix = os.path.splitext(uploaded_file.name)[-1].lower()
        if suffix in loaders:
            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
                tmp_file.write(uploaded_file.read())
                loader = loaders[suffix](tmp_file.name)
                file_docs = loader.load()
                for doc in file_docs:
                    doc.metadata["source"] = uploaded_file.name
                documents.extend(file_docs)
    return documents

def create_vectorstore(docs, db_path=DB_FOLDER):
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectordb = FAISS.from_documents(docs, embedding_model)
    vectordb.save_local(db_path)
    return vectordb

def load_vectorstore(db_path=DB_FOLDER):
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    faiss_index_path = os.path.join(db_path, "index.faiss")
    if not os.path.exists(faiss_index_path):
        st.warning(f"Kh√¥ng t√¨m th·∫•y ch·ªâ m·ª•c FAISS t·∫°i {faiss_index_path}. T·∫°o l·∫°i ch·ªâ m·ª•c t·ª´ t√†i li·ªáu...")
        raise FileNotFoundError(f"FAISS index file '{faiss_index_path}' not found.")
    vectordb = FAISS.load_local(db_path, embedding_model, allow_dangerous_deserialization=True)
    return vectordb

def create_gemini_llm():
    return GoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.2)

def create_qa_chain(llm, retriever):
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        input_key="question",
        output_key="answer"
    )
    return ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        return_source_documents=True,
    )

def get_fallback_answer(llm, query):
    return llm(query)

def delete_documents_from_vectorstore(sources_to_delete, db_path=DB_FOLDER):
    vectordb = load_vectorstore(db_path)
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    all_docs = vectordb.similarity_search("dummy query", k=1000)
    remaining_docs = [doc for doc in all_docs if doc.metadata.get("source") not in sources_to_delete]

    if not remaining_docs:
        shutil.rmtree(db_path)
        os.makedirs(db_path)
        return

    new_vectordb = FAISS.from_documents(remaining_docs, embedding_model)
    new_vectordb.save_local(db_path)

if not os.path.exists(DB_FOLDER):
    os.makedirs(DB_FOLDER)

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "qa_chain" not in st.session_state:
    try:
        vectordb = load_vectorstore()
        llm = create_gemini_llm()
        retriever = vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 3})
        st.session_state.qa_chain = create_qa_chain(llm, retriever)
    except FileNotFoundError:
        st.warning("Ch·ªâ m·ª•c FAISS ch∆∞a ƒë∆∞·ª£c t·∫°o. Vui l√≤ng t·∫£i t√†i li·ªáu v√† t·∫°o ch·ªâ m·ª•c tr∆∞·ªõc!")

# ------------------------ #
# Sidebar
# ------------------------ #
st.sidebar.header("T√πy ch·ªçn")
app_mode = st.sidebar.radio("Ch·ªçn ch·∫ø ƒë·ªô:", ["üìÇ Upload Documents", "üí¨ H·ªèi ƒë√°p", "üìú L·ªãch s·ª≠ h·ªèi ƒë√°p", "üóëÔ∏è X√≥a t√†i li·ªáu"])

# ------------------------ #
# Main App
# ------------------------ #
if app_mode == "üìÇ Upload Documents":
    st.subheader("üìÇ Upload t√†i li·ªáu m·ªõi v√†o h·ªá th·ªëng")
    uploaded_files = st.file_uploader("Ch·ªçn nhi·ªÅu file t√†i li·ªáu", type=["pdf", "txt", "docx", "md"], accept_multiple_files=True)

    if st.button("üîÑ X·ª≠ l√Ω v√† l∆∞u v√†o DB"):
        if uploaded_files:
            with st.spinner("ƒêang x·ª≠ l√Ω t√†i li·ªáu..."):
                docs = load_uploaded_documents(uploaded_files)
                splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
                split_docs = splitter.split_documents(docs)
                create_vectorstore(split_docs, db_path=DB_FOLDER)
            st.success("‚úÖ T√†i li·ªáu ƒë√£ x·ª≠ l√Ω v√† l∆∞u th√†nh c√¥ng!")
        else:
            st.warning("‚ö†Ô∏è B·∫°n ch∆∞a upload t√†i li·ªáu!")

elif app_mode == "üí¨ H·ªèi ƒë√°p":
    st.subheader("üí¨ ƒê·∫∑t c√¢u h·ªèi")

    try:
        vectordb = load_vectorstore()
        all_docs = vectordb.similarity_search("dummy query", k=1000)
        unique_sources = sorted(list(set(doc.metadata.get("source", "") for doc in all_docs)))

        selected_sources = st.multiselect("üìö Ch·ªçn t√†i li·ªáu b·∫°n mu·ªën search (kh√¥ng ch·ªçn = t·∫•t c·∫£):", unique_sources)
        query = st.text_input("üí¨ Nh·∫≠p c√¢u h·ªèi t·∫°i ƒë√¢y:", key="query_input")

        col1, col2 = st.columns([2, 1])
        with col1:
            search_button = st.button("üöÄ T√¨m c√¢u tr·∫£ l·ªùi", use_container_width=True)
        with col2:
            reset_button = st.button("‚ôªÔ∏è Reset h·ªôi tho·∫°i", use_container_width=True)

        if reset_button:
            st.session_state.qa_chain.memory.clear()
            st.session_state.chat_history.clear()
            st.success("‚úÖ ƒê√£ reset h·ªôi tho·∫°i!")

        if search_button or st.session_state.get('search_triggered', False):
            if query.strip():
                with st.spinner("ü§ñ ƒêang truy v·∫•n..."):
                    if selected_sources:
                        retriever = vectordb.as_retriever(
                            search_type="similarity",
                            search_kwargs={
                                "k": 3,
                                "filter": lambda d: d.metadata.get("source", "") in selected_sources
                            }
                        )
                    else:
                        retriever = vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 3})

                    qa_chain = create_qa_chain(create_gemini_llm(), retriever)
                    result = qa_chain.invoke({"question": query})

                    if result.get("answer"):
                        st.markdown("### üí¨ C√¢u tr·∫£ l·ªùi:")
                        st.write(result["answer"])

                        st.session_state.chat_history.append({
                            "question": query,
                            "answer": result["answer"],
                            "sources": result["source_documents"]
                        })

                        st.markdown("### üìö T√†i li·ªáu ngu·ªìn:")
                        for idx, doc in enumerate(result["source_documents"], 1):
                            st.write(f"**{idx}. T·ª´ t√†i li·ªáu:** `{doc.metadata.get('source', 'Kh√¥ng r√µ t√†i li·ªáu')}`")
                            with st.expander("Xem n·ªôi dung ƒëo·∫°n text"):
                                st.write(doc.page_content)
                    else:
                        st.markdown("### üí¨ C√¢u tr·∫£ l·ªùi t·ª´ Gemini:")
                        answer = get_fallback_answer(create_gemini_llm(), query)
                        st.write(answer)
                        st.session_state.chat_history.append({
                            "question": query,
                            "answer": answer,
                            "sources": []
                        })
            else:
                st.warning("‚ö†Ô∏è B·∫°n ch∆∞a nh·∫≠p c√¢u h·ªèi!")

    except FileNotFoundError as e:
        st.warning(f"L·ªói: {e}")

elif app_mode == "üìú L·ªãch s·ª≠ h·ªèi ƒë√°p":
    st.subheader("üìú Xem l·∫°i l·ªãch s·ª≠ h·ªèi ƒë√°p")
    if st.session_state.chat_history:  
        for idx, chat in enumerate(st.session_state.chat_history[::-1], 1):
            st.markdown(f"### {idx}. **C√¢u h·ªèi:** {chat['question']}")
            st.markdown(f"**Tr·∫£ l·ªùi:** {chat['answer']}")
            st.markdown("**Ngu·ªìn t√†i li·ªáu:**")
            for doc in chat['sources']:
                st.write(f"- {doc.metadata.get('source', 'Unknown')}")
            st.divider()
    else:
        st.info("‚ö°Ô∏è Ch∆∞a c√≥ l·ªãch s·ª≠ h·ªèi ƒë√°p n√†o.")

elif app_mode == "üóëÔ∏è X√≥a t√†i li·ªáu":
    st.subheader("üóëÔ∏è X√≥a t√†i li·ªáu kh·ªèi h·ªá th·ªëng")
    vectordb = load_vectorstore()
    all_docs = vectordb.similarity_search("dummy query", k=1000)
    document_sources = sorted(list(set(doc.metadata.get("source", "") for doc in all_docs)))

    selected_documents = st.multiselect("üìö Ch·ªçn t√†i li·ªáu b·∫°n mu·ªën x√≥a:", document_sources)

    if st.button("‚ùå X√≥a t√†i li·ªáu"):
        if selected_documents:
            with st.spinner("ƒêang x√≥a t√†i li·ªáu..."):
                delete_documents_from_vectorstore(selected_documents)
                st.success("‚úÖ T√†i li·ªáu ƒë√£ x√≥a kh·ªèi h·ªá th·ªëng!")
        else:
            st.warning("‚ö†Ô∏è B·∫°n ch∆∞a ch·ªçn t√†i li·ªáu n√†o ƒë·ªÉ x√≥a!")
