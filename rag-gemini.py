import os
import json
import tempfile
import streamlit as st

from langchain_community.document_loaders import (
    TextLoader, PDFMinerLoader, UnstructuredWordDocumentLoader, UnstructuredMarkdownLoader,
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai.llms import GoogleGenerativeAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_core.documents import Document

# ------------------------ #
# Configs
# ------------------------ #
GEMINI_API_KEY = "AIzaSyBUAYGkxXkMxQ_bQ6mqgwqCmEwieBRtD8c"  # <-- Thay báº±ng API KEY cá»§a báº¡n
os.environ["GOOGLE_API_KEY"] = GEMINI_API_KEY

DB_FOLDER = "faiss_index"

st.set_page_config(page_title="RAG FAISS + Gemini", page_icon="ðŸ”", layout="wide")
st.title("ðŸ” RAG App with Gemini")

# ------------------------ #
# Functions
# ------------------------ #

def load_uploaded_documents(uploaded_files):
    """Load nhiá»u tÃ i liá»‡u upload"""
    documents = []
    loaders = {
        ".txt": TextLoader,
        ".pdf": PDFMinerLoader,
        ".docx": UnstructuredWordDocumentLoader,
        ".md": UnstructuredMarkdownLoader
    }
    for uploaded_file in uploaded_files:
        suffix = os.path.splitext(uploaded_file.name)[-1].lower()
        if suffix in loaders:
            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
                tmp_file.write(uploaded_file.read())
                loader = loaders[suffix](tmp_file.name)
                file_docs = loader.load()
                for doc in file_docs:
                    doc.metadata["source"] = uploaded_file.name
                documents.extend(file_docs)
    return documents

def create_vectorstore(docs, db_path=DB_FOLDER):
    """Táº¡o FAISS database"""
    embedding_model = HuggingFaceEmbeddings(model_name="Snowflake/snowflake-arctic-embed-l-v2.0")
    vectordb = FAISS.from_documents(docs, embedding_model)
    vectordb.save_local(db_path)
    return vectordb

def load_vectorstore(db_path=DB_FOLDER):
    """Load FAISS database"""
    embedding_model = HuggingFaceEmbeddings(model_name="Snowflake/snowflake-arctic-embed-l-v2.0")
    vectordb = FAISS.load_local(db_path, embedding_model, allow_dangerous_deserialization=True)
    return vectordb

def create_gemini_llm():
    """Táº¡o Gemini model"""
    return GoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.2)

def create_qa_chain(llm, retriever):
    """Táº¡o QA chain"""
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        input_key="question",
        output_key="answer"  # Chá»‰ Ä‘á»‹nh rÃµ rÃ ng 'output_key' lÃ  'answer'
    )
    return ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        return_source_documents=True,
    )

def get_fallback_answer(llm, query):
    """Tráº£ lá»i fallback khi khÃ´ng cÃ³ tÃ i liá»‡u liÃªn quan"""
    return llm(query)

def delete_documents_from_vectorstore(doc_ids_to_delete, db_path=DB_FOLDER):
    """XÃ³a tÃ i liá»‡u khá»i FAISS database"""
    vectordb = load_vectorstore(db_path)
    vectordb.delete_documents(doc_ids_to_delete)
    vectordb.save_local(db_path)


if not os.path.exists(DB_FOLDER):
    os.makedirs(DB_FOLDER)

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "qa_chain" not in st.session_state:
    vectordb = load_vectorstore()
    llm = create_gemini_llm()
    retriever = vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 3})
    st.session_state.qa_chain = create_qa_chain(llm, retriever)

# ------------------------ #
# Sidebar
# ------------------------ #
st.sidebar.header("TÃ¹y chá»n")
app_mode = st.sidebar.radio("Chá»n cháº¿ Ä‘á»™:", ["ðŸ“‚ Upload Documents", "ðŸ’¬ Há»i Ä‘Ã¡p", "ðŸ“œ Lá»‹ch sá»­ há»i Ä‘Ã¡p", "ðŸ—‘ï¸ XÃ³a tÃ i liá»‡u"])

# ------------------------ #
# Main App
# ------------------------ #
if app_mode == "ðŸ“‚ Upload Documents":
    st.subheader("ðŸ“‚ Upload tÃ i liá»‡u má»›i vÃ o há»‡ thá»‘ng")

    uploaded_files = st.file_uploader(
        "Chá»n nhiá»u file tÃ i liá»‡u",
        type=["pdf", "txt", "docx", "md"],
        accept_multiple_files=True
    )

    if st.button("ðŸ”„ Xá»­ lÃ½ vÃ  lÆ°u vÃ o DB"):
        if uploaded_files:
            with st.spinner("Äang xá»­ lÃ½ tÃ i liá»‡u..."):
                docs = load_uploaded_documents(uploaded_files)
                splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
                split_docs = splitter.split_documents(docs)
                create_vectorstore(split_docs, db_path=DB_FOLDER)
            st.success("âœ… TÃ i liá»‡u Ä‘Ã£ xá»­ lÃ½ vÃ  lÆ°u thÃ nh cÃ´ng!")
        else:
            st.warning("âš ï¸ Báº¡n chÆ°a upload tÃ i liá»‡u!")

elif app_mode == "ðŸ’¬ Há»i Ä‘Ã¡p":
    st.subheader("ðŸ’¬ Äáº·t cÃ¢u há»i")

    vectordb = load_vectorstore()

    # Láº¥y danh sÃ¡ch táº¥t cáº£ tÃ i liá»‡u
    all_docs = vectordb.similarity_search("dummy query", k=1000)
    unique_sources = sorted(list(set(doc.metadata.get("source", "") for doc in all_docs)))

    selected_sources = st.multiselect(
        "ðŸ“š Chá»n tÃ i liá»‡u báº¡n muá»‘n search (khÃ´ng chá»n = tÃ¬m trong táº¥t cáº£ tÃ i liá»‡u):", 
        unique_sources
    )

    query = st.text_input("ðŸ’¬ Nháº­p cÃ¢u há»i táº¡i Ä‘Ã¢y:", key="query_input")

    col1, col2 = st.columns([2, 1])
    with col1:
        search_button = st.button("ðŸš€ TÃ¬m cÃ¢u tráº£ lá»i", use_container_width=True)
    with col2:
        reset_button = st.button("â™»ï¸ Reset há»™i thoáº¡i", use_container_width=True)

    if reset_button:
        st.session_state.qa_chain.memory.clear()
        st.session_state.chat_history.clear()
        st.success("âœ… ÄÃ£ reset há»™i thoáº¡i!")

    # Äiá»u kiá»‡n thá»±c hiá»‡n tÃ¬m kiáº¿m chá»‰ khi nháº¥n nÃºt hoáº·c Enter
    if search_button or st.session_state.get('search_triggered', False):
        if query.strip():
            with st.spinner("ðŸ¤– Äang truy váº¥n..."):
                if selected_sources:
                    retriever = vectordb.as_retriever(
                        search_type="similarity",
                        search_kwargs={
                            "k": 3,
                            "filter": lambda d: d.metadata.get("source", "") in selected_sources
                        }
                    )
                else:
                    retriever = vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 3})

                # Táº¡o chain má»›i táº¡m thá»i cho cÃ¢u há»i nÃ y
                qa_chain = create_qa_chain(create_gemini_llm(), retriever)
                result = qa_chain.invoke({"question": query})

                if result.get("answer"):
                    # Náº¿u cÃ³ cÃ¢u tráº£ lá»i tá»« tÃ i liá»‡u
                    st.markdown("### ðŸ’¬ CÃ¢u tráº£ lá»i:")
                    st.write(result["answer"])

                    # LÆ°u lá»‹ch sá»­
                    st.session_state.chat_history.append({
                        "question": query,
                        "answer": result["answer"],
                        "sources": result["source_documents"]
                    })

                    st.markdown("### ðŸ“š TÃ i liá»‡u nguá»“n:")
                    for idx, doc in enumerate(result["source_documents"], 1):
                        st.write(f"**{idx}. Tá»« tÃ i liá»‡u:** `{doc.metadata.get('source', 'KhÃ´ng rÃµ tÃ i liá»‡u')}`")
                        with st.expander("Xem ná»™i dung Ä‘oáº¡n text"):
                            st.write(doc.page_content)
                else:
                    # Náº¿u khÃ´ng cÃ³ cÃ¢u tráº£ lá»i tá»« tÃ i liá»‡u, tráº£ lá»i tá»« mÃ´ hÃ¬nh
                    st.markdown("### ðŸ’¬ CÃ¢u tráº£ lá»i tá»« Gemini:")
                    answer = get_fallback_answer(create_gemini_llm(), query)
                    st.write(answer)
                    st.session_state.chat_history.append({
                        "question": query,
                        "answer": answer,
                        "sources": []  # KhÃ´ng cÃ³ nguá»“n tÃ i liá»‡u
                    })

        else:
            st.warning("âš ï¸ Báº¡n chÆ°a nháº­p cÃ¢u há»i!")

elif app_mode == "ðŸ“œ Lá»‹ch sá»­ há»i Ä‘Ã¡p":
    st.subheader("ðŸ“œ Xem láº¡i lá»‹ch sá»­ há»i Ä‘Ã¡p")

    if st.session_state.chat_history:  
        for idx, chat in enumerate(st.session_state.chat_history[::-1], 1):
            st.markdown(f"### {idx}. **CÃ¢u há»i:** {chat['question']}")
            st.markdown(f"**Tráº£ lá»i:** {chat['answer']}")
            st.markdown("**Nguá»“n tÃ i liá»‡u:**")
            for doc in chat['sources']:
                st.write(f"- {doc.metadata.get('source', 'Unknown')}")
            st.divider()
    else:
        st.info("âš¡ï¸ ChÆ°a cÃ³ lá»‹ch sá»­ há»i Ä‘Ã¡p nÃ o.")

elif app_mode == "ðŸ—‘ï¸ XÃ³a tÃ i liá»‡u":
    st.subheader("ðŸ—‘ï¸ XÃ³a tÃ i liá»‡u khá»i há»‡ thá»‘ng")

    # Load vectorstore vÃ  láº¥y danh sÃ¡ch tÃ i liá»‡u
    vectordb = load_vectorstore()

    # Láº¥y danh sÃ¡ch táº¥t cáº£ tÃ i liá»‡u
    all_docs = vectordb.similarity_search("dummy query", k=1000)
    document_sources = sorted(list(set(doc.metadata.get("source", "") for doc in all_docs)))

    # Cho phÃ©p ngÆ°á»i dÃ¹ng chá»n tÃ i liá»‡u Ä‘á»ƒ xÃ³a
    selected_documents = st.multiselect(
        "ðŸ“š Chá»n tÃ i liá»‡u báº¡n muá»‘n xÃ³a:", 
        document_sources
    )

    if st.button("âŒ XÃ³a tÃ i liá»‡u"):
        if selected_documents:
            with st.spinner("Äang xÃ³a tÃ i liá»‡u..."):
                # Táº¡o danh sÃ¡ch cÃ¡c tÃ i liá»‡u cáº§n xÃ³a
                doc_ids_to_delete = [doc.metadata["source"] for doc in all_docs if doc.metadata.get("source") in selected_documents]
                delete_documents_from_vectorstore(doc_ids_to_delete)
                st.success("âœ… TÃ i liá»‡u Ä‘Ã£ xÃ³a khá»i há»‡ thá»‘ng!")
        else:
            st.warning("âš ï¸ Báº¡n chÆ°a chá»n tÃ i liá»‡u nÃ o Ä‘á»ƒ xÃ³a!")
